{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ddadcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TensorBoard logs: runs/training_20251211_164459\n",
      "Utilisation de : cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Cr√©er un writer avec timestamp pour identifier les runs\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f'runs/training_{timestamp}')\n",
    "print(f\"üìä TensorBoard logs: runs/training_{timestamp}\")\n",
    "\n",
    "# Configuration du device (GPU si dispo, sinon CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Utilisation de : {device}\")\n",
    "\n",
    "# Transformations : Conversion en Tensor + Normalisation\n",
    "# On ajoute une fonction lambda pour d√©caler les labels de 1 (1-26 -> 0-25)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.RandomHorizontalFlip(1),\n",
    "    transforms.RandomRotation(90)\n",
    "     # Normalisation moyenne 0.5, std 0.5\n",
    "])\n",
    "\n",
    "# Fonction pour corriger les labels (target - 1)\n",
    "def target_transform(target):\n",
    "    return target - 1\n",
    "\n",
    "# T√©l√©chargement du dataset EMNIST (Lettres)\n",
    "train_dataset = torchvision.datasets.EMNIST(\n",
    "    root='./data', \n",
    "    split='letters', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.EMNIST(\n",
    "    root='./data', \n",
    "    split='letters', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "# Cr√©ation des DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d74e73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_MLP_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_MLP_Network, self).__init__()\n",
    "        \n",
    "        # --- PARTIE 1 : Extraction de Features (CNN) ---\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # Conv Layer 1 : Input 1 canal (gris), 32 filtres\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Image devient 14x14\n",
    "            \n",
    "            # Conv Layer 2 : Input 32, 64 filtres\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Image devient 7x7\n",
    "        )\n",
    "        \n",
    "        # --- PARTIE 2 : Classification (MLP) ---\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Flatten(), # Aplatit les donn√©es (64 * 7 * 7)\n",
    "            \n",
    "            # Couche cach√©e du MLP (Dense layer)\n",
    "            nn.Linear(64 * 7 * 7, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # Pour √©viter le surapprentissage\n",
    "            \n",
    "            # Couche de sortie (26 lettres)\n",
    "            nn.Linear(512, 26) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passage dans les filtres CNN\n",
    "        x = self.cnn_layers(x)\n",
    "        # Passage dans le MLP\n",
    "        x = self.mlp_layers(x)\n",
    "        return x\n",
    "\n",
    "# Initialisation du mod√®le\n",
    "\n",
    "model = CNN_MLP_Network().to(torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2826b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©but de l'entra√Ænement...\n",
      "\n",
      "Epoch [1/5], Step [100/1950], Loss: 2.5247\n",
      "Epoch [1/5], Step [100/1950], Loss: 2.5247\n",
      "Epoch [1/5], Step [200/1950], Loss: 1.8078\n",
      "Epoch [1/5], Step [200/1950], Loss: 1.8078\n",
      "Epoch [1/5], Step [300/1950], Loss: 1.2510\n",
      "Epoch [1/5], Step [300/1950], Loss: 1.2510\n",
      "Epoch [1/5], Step [400/1950], Loss: 1.3366\n",
      "Epoch [1/5], Step [400/1950], Loss: 1.3366\n",
      "Epoch [1/5], Step [500/1950], Loss: 1.3538\n",
      "Epoch [1/5], Step [500/1950], Loss: 1.3538\n",
      "Epoch [1/5], Step [600/1950], Loss: 1.3053\n",
      "Epoch [1/5], Step [600/1950], Loss: 1.3053\n",
      "Epoch [1/5], Step [700/1950], Loss: 0.9797\n",
      "Epoch [1/5], Step [700/1950], Loss: 0.9797\n",
      "Epoch [1/5], Step [800/1950], Loss: 1.3639\n",
      "Epoch [1/5], Step [800/1950], Loss: 1.3639\n",
      "Epoch [1/5], Step [900/1950], Loss: 1.2051\n",
      "Epoch [1/5], Step [900/1950], Loss: 1.2051\n",
      "Epoch [1/5], Step [1000/1950], Loss: 0.9854\n",
      "Epoch [1/5], Step [1000/1950], Loss: 0.9854\n",
      "Epoch [1/5], Step [1100/1950], Loss: 1.0535\n",
      "Epoch [1/5], Step [1100/1950], Loss: 1.0535\n",
      "Epoch [1/5], Step [1200/1950], Loss: 0.8742\n",
      "Epoch [1/5], Step [1200/1950], Loss: 0.8742\n",
      "Epoch [1/5], Step [1300/1950], Loss: 0.9735\n",
      "Epoch [1/5], Step [1300/1950], Loss: 0.9735\n",
      "Epoch [1/5], Step [1400/1950], Loss: 0.8118\n",
      "Epoch [1/5], Step [1400/1950], Loss: 0.8118\n",
      "Epoch [1/5], Step [1500/1950], Loss: 0.8812\n",
      "Epoch [1/5], Step [1500/1950], Loss: 0.8812\n",
      "Epoch [1/5], Step [1600/1950], Loss: 0.6946\n",
      "Epoch [1/5], Step [1600/1950], Loss: 0.6946\n",
      "Epoch [1/5], Step [1700/1950], Loss: 0.8416\n",
      "Epoch [1/5], Step [1700/1950], Loss: 0.8416\n",
      "Epoch [1/5], Step [1800/1950], Loss: 1.0889\n",
      "Epoch [1/5], Step [1800/1950], Loss: 1.0889\n",
      "Epoch [1/5], Step [1900/1950], Loss: 0.7366\n",
      "Epoch [1/5], Step [1900/1950], Loss: 0.7366\n",
      "Epoch [1/5] ‚Üí Test Accuracy: 80.68% (16782/20800)\n",
      "\n",
      "Epoch [1/5] ‚Üí Test Accuracy: 80.68% (16782/20800)\n",
      "\n",
      "Epoch [2/5], Step [100/1950], Loss: 0.5803\n",
      "Epoch [2/5], Step [100/1950], Loss: 0.5803\n",
      "Epoch [2/5], Step [200/1950], Loss: 0.6354\n",
      "Epoch [2/5], Step [200/1950], Loss: 0.6354\n",
      "Epoch [2/5], Step [300/1950], Loss: 0.6135\n",
      "Epoch [2/5], Step [300/1950], Loss: 0.6135\n",
      "Epoch [2/5], Step [400/1950], Loss: 0.8961\n",
      "Epoch [2/5], Step [400/1950], Loss: 0.8961\n",
      "Epoch [2/5], Step [500/1950], Loss: 0.4854\n",
      "Epoch [2/5], Step [500/1950], Loss: 0.4854\n",
      "Epoch [2/5], Step [600/1950], Loss: 0.8861\n",
      "Epoch [2/5], Step [600/1950], Loss: 0.8861\n",
      "Epoch [2/5], Step [700/1950], Loss: 0.7404\n",
      "Epoch [2/5], Step [700/1950], Loss: 0.7404\n",
      "Epoch [2/5], Step [800/1950], Loss: 0.9263\n",
      "Epoch [2/5], Step [800/1950], Loss: 0.9263\n",
      "Epoch [2/5], Step [900/1950], Loss: 0.6924\n",
      "Epoch [2/5], Step [900/1950], Loss: 0.6924\n",
      "Epoch [2/5], Step [1000/1950], Loss: 0.9415\n",
      "Epoch [2/5], Step [1000/1950], Loss: 0.9415\n",
      "Epoch [2/5], Step [1100/1950], Loss: 0.5582\n",
      "Epoch [2/5], Step [1100/1950], Loss: 0.5582\n",
      "Epoch [2/5], Step [1200/1950], Loss: 0.7376\n",
      "Epoch [2/5], Step [1200/1950], Loss: 0.7376\n",
      "Epoch [2/5], Step [1300/1950], Loss: 0.8741\n",
      "Epoch [2/5], Step [1300/1950], Loss: 0.8741\n",
      "Epoch [2/5], Step [1400/1950], Loss: 0.7050\n",
      "Epoch [2/5], Step [1400/1950], Loss: 0.7050\n",
      "Epoch [2/5], Step [1500/1950], Loss: 0.7502\n",
      "Epoch [2/5], Step [1500/1950], Loss: 0.7502\n",
      "Epoch [2/5], Step [1600/1950], Loss: 0.7673\n",
      "Epoch [2/5], Step [1600/1950], Loss: 0.7673\n",
      "Epoch [2/5], Step [1700/1950], Loss: 0.4988\n",
      "Epoch [2/5], Step [1700/1950], Loss: 0.4988\n",
      "Epoch [2/5], Step [1800/1950], Loss: 0.6511\n",
      "Epoch [2/5], Step [1800/1950], Loss: 0.6511\n",
      "Epoch [2/5], Step [1900/1950], Loss: 0.7332\n",
      "Epoch [2/5], Step [1900/1950], Loss: 0.7332\n",
      "Epoch [2/5] ‚Üí Test Accuracy: 84.55% (17586/20800)\n",
      "\n",
      "Epoch [2/5] ‚Üí Test Accuracy: 84.55% (17586/20800)\n",
      "\n",
      "Epoch [3/5], Step [100/1950], Loss: 0.5105\n",
      "Epoch [3/5], Step [100/1950], Loss: 0.5105\n",
      "Epoch [3/5], Step [200/1950], Loss: 0.7400\n",
      "Epoch [3/5], Step [200/1950], Loss: 0.7400\n",
      "Epoch [3/5], Step [300/1950], Loss: 0.8172\n",
      "Epoch [3/5], Step [300/1950], Loss: 0.8172\n",
      "Epoch [3/5], Step [400/1950], Loss: 0.6937\n",
      "Epoch [3/5], Step [400/1950], Loss: 0.6937\n",
      "Epoch [3/5], Step [500/1950], Loss: 0.7769\n",
      "Epoch [3/5], Step [500/1950], Loss: 0.7769\n",
      "Epoch [3/5], Step [600/1950], Loss: 0.5127\n",
      "Epoch [3/5], Step [600/1950], Loss: 0.5127\n",
      "Epoch [3/5], Step [700/1950], Loss: 0.6620\n",
      "Epoch [3/5], Step [700/1950], Loss: 0.6620\n",
      "Epoch [3/5], Step [800/1950], Loss: 0.6924\n",
      "Epoch [3/5], Step [800/1950], Loss: 0.6924\n",
      "Epoch [3/5], Step [900/1950], Loss: 0.5538\n",
      "Epoch [3/5], Step [900/1950], Loss: 0.5538\n",
      "Epoch [3/5], Step [1000/1950], Loss: 0.5418\n",
      "Epoch [3/5], Step [1000/1950], Loss: 0.5418\n",
      "Epoch [3/5], Step [1100/1950], Loss: 0.7234\n",
      "Epoch [3/5], Step [1100/1950], Loss: 0.7234\n",
      "Epoch [3/5], Step [1200/1950], Loss: 0.6091\n",
      "Epoch [3/5], Step [1200/1950], Loss: 0.6091\n",
      "Epoch [3/5], Step [1300/1950], Loss: 0.7906\n",
      "Epoch [3/5], Step [1300/1950], Loss: 0.7906\n",
      "Epoch [3/5], Step [1400/1950], Loss: 0.5282\n",
      "Epoch [3/5], Step [1400/1950], Loss: 0.5282\n",
      "Epoch [3/5], Step [1500/1950], Loss: 0.8274\n",
      "Epoch [3/5], Step [1500/1950], Loss: 0.8274\n",
      "Epoch [3/5], Step [1600/1950], Loss: 0.5521\n",
      "Epoch [3/5], Step [1600/1950], Loss: 0.5521\n",
      "Epoch [3/5], Step [1700/1950], Loss: 0.5475\n",
      "Epoch [3/5], Step [1700/1950], Loss: 0.5475\n",
      "Epoch [3/5], Step [1800/1950], Loss: 0.5810\n",
      "Epoch [3/5], Step [1800/1950], Loss: 0.5810\n",
      "Epoch [3/5], Step [1900/1950], Loss: 0.7786\n",
      "Epoch [3/5], Step [1900/1950], Loss: 0.7786\n",
      "Epoch [3/5] ‚Üí Test Accuracy: 86.22% (17933/20800)\n",
      "\n",
      "Epoch [3/5] ‚Üí Test Accuracy: 86.22% (17933/20800)\n",
      "\n",
      "Epoch [4/5], Step [100/1950], Loss: 0.6404\n",
      "Epoch [4/5], Step [100/1950], Loss: 0.6404\n",
      "Epoch [4/5], Step [200/1950], Loss: 0.3807\n",
      "Epoch [4/5], Step [200/1950], Loss: 0.3807\n",
      "Epoch [4/5], Step [300/1950], Loss: 0.5695\n",
      "Epoch [4/5], Step [300/1950], Loss: 0.5695\n",
      "Epoch [4/5], Step [400/1950], Loss: 1.0433\n",
      "Epoch [4/5], Step [400/1950], Loss: 1.0433\n",
      "Epoch [4/5], Step [500/1950], Loss: 0.6735\n",
      "Epoch [4/5], Step [500/1950], Loss: 0.6735\n",
      "Epoch [4/5], Step [600/1950], Loss: 0.6109\n",
      "Epoch [4/5], Step [600/1950], Loss: 0.6109\n",
      "Epoch [4/5], Step [700/1950], Loss: 0.5666\n",
      "Epoch [4/5], Step [700/1950], Loss: 0.5666\n",
      "Epoch [4/5], Step [800/1950], Loss: 0.5068\n",
      "Epoch [4/5], Step [800/1950], Loss: 0.5068\n",
      "Epoch [4/5], Step [900/1950], Loss: 0.3592\n",
      "Epoch [4/5], Step [900/1950], Loss: 0.3592\n",
      "Epoch [4/5], Step [1000/1950], Loss: 0.4834\n",
      "Epoch [4/5], Step [1000/1950], Loss: 0.4834\n",
      "Epoch [4/5], Step [1100/1950], Loss: 0.5356\n",
      "Epoch [4/5], Step [1100/1950], Loss: 0.5356\n",
      "Epoch [4/5], Step [1200/1950], Loss: 0.7461\n",
      "Epoch [4/5], Step [1200/1950], Loss: 0.7461\n",
      "Epoch [4/5], Step [1300/1950], Loss: 0.5849\n",
      "Epoch [4/5], Step [1300/1950], Loss: 0.5849\n",
      "Epoch [4/5], Step [1400/1950], Loss: 0.3931\n",
      "Epoch [4/5], Step [1400/1950], Loss: 0.3931\n",
      "Epoch [4/5], Step [1500/1950], Loss: 0.7746\n",
      "Epoch [4/5], Step [1500/1950], Loss: 0.7746\n",
      "Epoch [4/5], Step [1600/1950], Loss: 0.5656\n",
      "Epoch [4/5], Step [1600/1950], Loss: 0.5656\n",
      "Epoch [4/5], Step [1700/1950], Loss: 0.7560\n",
      "Epoch [4/5], Step [1700/1950], Loss: 0.7560\n",
      "Epoch [4/5], Step [1800/1950], Loss: 0.4150\n",
      "Epoch [4/5], Step [1800/1950], Loss: 0.4150\n",
      "Epoch [4/5], Step [1900/1950], Loss: 0.2876\n",
      "Epoch [4/5], Step [1900/1950], Loss: 0.2876\n",
      "Epoch [4/5] ‚Üí Test Accuracy: 87.31% (18160/20800)\n",
      "\n",
      "Epoch [4/5] ‚Üí Test Accuracy: 87.31% (18160/20800)\n",
      "\n",
      "Epoch [5/5], Step [100/1950], Loss: 0.4948\n",
      "Epoch [5/5], Step [100/1950], Loss: 0.4948\n",
      "Epoch [5/5], Step [200/1950], Loss: 0.5194\n",
      "Epoch [5/5], Step [200/1950], Loss: 0.5194\n",
      "Epoch [5/5], Step [300/1950], Loss: 0.6217\n",
      "Epoch [5/5], Step [300/1950], Loss: 0.6217\n",
      "Epoch [5/5], Step [400/1950], Loss: 0.7199\n",
      "Epoch [5/5], Step [400/1950], Loss: 0.7199\n",
      "Epoch [5/5], Step [500/1950], Loss: 0.3662\n",
      "Epoch [5/5], Step [500/1950], Loss: 0.3662\n",
      "Epoch [5/5], Step [600/1950], Loss: 0.4378\n",
      "Epoch [5/5], Step [600/1950], Loss: 0.4378\n",
      "Epoch [5/5], Step [700/1950], Loss: 0.6947\n",
      "Epoch [5/5], Step [700/1950], Loss: 0.6947\n",
      "Epoch [5/5], Step [800/1950], Loss: 0.5780\n",
      "Epoch [5/5], Step [800/1950], Loss: 0.5780\n",
      "Epoch [5/5], Step [900/1950], Loss: 0.6877\n",
      "Epoch [5/5], Step [900/1950], Loss: 0.6877\n",
      "Epoch [5/5], Step [1000/1950], Loss: 0.3097\n",
      "Epoch [5/5], Step [1000/1950], Loss: 0.3097\n",
      "Epoch [5/5], Step [1100/1950], Loss: 0.4642\n",
      "Epoch [5/5], Step [1100/1950], Loss: 0.4642\n",
      "Epoch [5/5], Step [1200/1950], Loss: 0.6570\n",
      "Epoch [5/5], Step [1200/1950], Loss: 0.6570\n",
      "Epoch [5/5], Step [1300/1950], Loss: 0.2653\n",
      "Epoch [5/5], Step [1300/1950], Loss: 0.2653\n",
      "Epoch [5/5], Step [1400/1950], Loss: 0.5579\n",
      "Epoch [5/5], Step [1400/1950], Loss: 0.5579\n",
      "Epoch [5/5], Step [1500/1950], Loss: 0.8104\n",
      "Epoch [5/5], Step [1500/1950], Loss: 0.8104\n",
      "Epoch [5/5], Step [1600/1950], Loss: 0.6658\n",
      "Epoch [5/5], Step [1600/1950], Loss: 0.6658\n",
      "Epoch [5/5], Step [1700/1950], Loss: 0.4565\n",
      "Epoch [5/5], Step [1700/1950], Loss: 0.4565\n",
      "Epoch [5/5], Step [1800/1950], Loss: 0.6167\n",
      "Epoch [5/5], Step [1800/1950], Loss: 0.6167\n",
      "Epoch [5/5], Step [1900/1950], Loss: 0.6868\n",
      "Epoch [5/5], Step [1900/1950], Loss: 0.6868\n",
      "Epoch [5/5] ‚Üí Test Accuracy: 87.86% (18275/20800)\n",
      "\n",
      "‚úÖ Entra√Ænement termin√© !\n",
      "üìä TensorBoard logs sauvegard√©s\n",
      "Epoch [5/5] ‚Üí Test Accuracy: 87.86% (18275/20800)\n",
      "\n",
      "‚úÖ Entra√Ænement termin√© !\n",
      "üìä TensorBoard logs sauvegard√©s\n"
     ]
    }
   ],
   "source": [
    "# Hyperparam√®tres\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5 # Tu peux augmenter ce nombre pour de meilleurs r√©sultats\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"D√©but de l'entra√Ænement...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- ENTRA√éNEMENT ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass et optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            avg_loss = running_loss / 100\n",
    "            writer.add_scalar('training loss', avg_loss, epoch * len(train_loader) + i)\n",
    "            running_loss = 0.0\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # --- √âVALUATION √Ä LA FIN DE CHAQUE EPOCH ---\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    writer.add_scalar('test accuracy', accuracy, epoch)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] ‚Üí Test Accuracy: {accuracy:.2f}% ({correct}/{total})\\n')\n",
    "\n",
    "print(\"‚úÖ Entra√Ænement termin√© !\")\n",
    "writer.close()\n",
    "print(\"üìä TensorBoard logs sauvegard√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17f983cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supprim√© : mon_modele_v2.onnx.data\n",
      "üîß Export ONNX avec tous les poids en dur...\n",
      "[torch.onnx] Obtain model graph for `CNN_MLP_Network([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `CNN_MLP_Network([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Obtain model graph for `CNN_MLP_Network([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `CNN_MLP_Network([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ‚úÖ\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n",
      "‚úÖ Fichier cr√©√©. Optimisation en cours...\n",
      "‚úÖ 'mon_modele_v2.onnx' optimis√© et sauvegard√©!\n",
      "   ‚úì Tous les poids int√©gr√©s dans le fichier unique\n",
      "   ‚úì Aucun fichier .data externe\n",
      "‚ö†Ô∏è Fichiers .data restants : ['mon_modele_v2.onnx.data']\n",
      "[torch.onnx] Run decomposition... ‚úÖ\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n",
      "‚úÖ Fichier cr√©√©. Optimisation en cours...\n",
      "‚úÖ 'mon_modele_v2.onnx' optimis√© et sauvegard√©!\n",
      "   ‚úì Tous les poids int√©gr√©s dans le fichier unique\n",
      "   ‚úì Aucun fichier .data externe\n",
      "‚ö†Ô∏è Fichiers .data restants : ['mon_modele_v2.onnx.data']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import os\n",
    "\n",
    "# üóëÔ∏è Nettoyer les anciens fichiers .data\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".onnx.data\"):\n",
    "        os.remove(file)\n",
    "        print(f\"Supprim√© : {file}\")\n",
    "\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "print(\"üîß Export ONNX avec tous les poids en dur...\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"mon_modele_v2.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=18,  # Version plus r√©cente et compatible\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fichier cr√©√©. Optimisation en cours...\")\n",
    "\n",
    "\n",
    "onnx_model = onnx.load(\"mon_modele_v2.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "\n",
    "for initializer in onnx_model.graph.initializer:\n",
    "    if initializer.HasField(\"raw_data\"):\n",
    "        continue  # D√©j√† en dur\n",
    "    \n",
    "onnx.save_model(onnx_model, \"mon_modele_v2.onnx\", save_as_external_data=False)\n",
    "\n",
    "print(\"‚úÖ 'mon_modele_v2.onnx' optimis√© et sauvegard√©!\")\n",
    "print(\"   ‚úì Tous les poids int√©gr√©s dans le fichier unique\")\n",
    "print(\"   ‚úì Aucun fichier .data externe\")\n",
    "\n",
    "onnx_data_files = [f for f in os.listdir(\".\") if f.endswith(\".onnx.data\")]\n",
    "if onnx_data_files:\n",
    "    print(f\"‚ö†Ô∏è Fichiers .data restants : {onnx_data_files}\")\n",
    "else:\n",
    "    print(\"‚úì Pas de fichiers .data externes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19e9cad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "           8,  32,  37,  37,  37,  37,  37,  20,   7,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  22,\n",
       "          46, 114, 127, 127, 127, 127, 125,  77,  32,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,  22,  46, 115, 172,\n",
       "         208, 245, 250, 250, 250, 250, 249, 206, 126,   8,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   3,  36, 159, 207, 245, 252,\n",
       "         254, 254, 254, 255, 255, 254, 254, 245, 204,  34,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   7,  22,  95, 218, 244, 254, 254,\n",
       "         254, 254, 255, 255, 255, 255, 254, 250, 220,  50,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   1,   8,  90, 159, 232, 253, 254, 254, 251,\n",
       "         250, 250, 250, 250, 250, 254, 254, 254, 245, 114,   4,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  20,  77, 218, 245, 254, 254, 254, 234, 145,\n",
       "         129, 127, 127, 127, 141, 222, 247, 254, 250, 129,   5,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  34, 115, 243, 253, 254, 252, 250, 207,  64,\n",
       "          41,  38,  41,  40,  53, 176, 232, 254, 250, 141,  11,   1,   0,   0],\n",
       "        [  0,   0,   0,   0,   3,  84, 172, 252, 254, 251, 189, 144,  77,  11,\n",
       "          14,  38,  99,  76,  12,  84, 172, 252, 254, 220,  77,  20,   0,   0],\n",
       "        [  0,   0,   0,   0,  21, 170, 232, 254, 254, 220,  84,  40,  30,  19,\n",
       "          52, 170, 236, 213,  66,  36, 115, 245, 254, 249, 125,  37,   0,   0],\n",
       "        [  0,   0,   0,   0,  34, 204, 245, 254, 252, 177,  47,  52,  75,  22,\n",
       "          47, 207, 250, 231,  82,  23,  82, 233, 252, 250, 127,  37,   0,   0],\n",
       "        [  0,   0,   0,   2,  82, 233, 252, 254, 251, 143,  18,  19,  27,  26,\n",
       "          83, 233, 254, 246, 115,   8,  34, 204, 245, 250, 127,  37,   0,   0],\n",
       "        [  0,   0,   0,   4, 125, 249, 254, 254, 254, 221, 107,  51,   2,  37,\n",
       "         125, 249, 254, 250, 129,  10,  14, 143, 222, 249, 125,  37,   0,   0],\n",
       "        [  0,   0,   0,   5, 129, 250, 254, 254, 254, 233,  92,  32,   0,  37,\n",
       "         127, 250, 254, 252, 191, 129, 129, 191, 236, 233,  82,  21,   0,   0],\n",
       "        [  0,   0,   0,   9, 140, 250, 254, 255, 254, 222,  52,  11,   0,  39,\n",
       "         129, 250, 254, 254, 236, 218, 218, 236, 249, 220,  50,   9,   0,   0],\n",
       "        [  0,   0,   4,  32, 203, 254, 254, 255, 254, 217,  42,  13,  32, 101,\n",
       "         177, 252, 254, 254, 254, 254, 254, 254, 251, 170,  21,   2,   0,   0],\n",
       "        [  0,   0,   4,  32, 203, 254, 254, 254, 254, 229, 129, 117, 140, 212,\n",
       "         240, 254, 255, 254, 254, 254, 254, 248, 222,  79,   3,   0,   0,   0],\n",
       "        [  0,   0,   2,  21, 174, 252, 254, 254, 253, 217, 151, 148, 101, 174,\n",
       "         222, 254, 254, 254, 254, 250, 250, 236, 188,  38,   0,   0,   0,   0],\n",
       "        [  0,   0,   2,  21, 172, 252, 254, 247, 221,  92,  26,  27,   8,  53,\n",
       "         141, 250, 254, 254, 222, 141, 130, 152, 128,  16,   0,   0,   0,   0],\n",
       "        [  0,   0,   2,  20, 170, 252, 253, 207, 127,  10,   0,   0,   0,  37,\n",
       "         125, 249, 254, 250, 139,  13,   6,  16,  15,   1,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   9, 140, 250, 247, 159,  79,   3,   0,   0,   0,  32,\n",
       "         113, 243, 253, 243, 115,   4,   0,   1,   1,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   4, 125, 237, 206,  47,  10,   0,   0,   0,   0,   7,\n",
       "          33, 158, 200, 158,  33,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   2,  63, 111,  76,   7,   0,   0,   0,   0,   0,   0,\n",
       "           1,  20,  32,  20,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  18,  32,  20,   1,   0,   0,   0,   0,   0,   0,\n",
       "           0,   2,   3,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
